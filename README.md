# Nombres completos:
## - Edson Bryan BÃ©jar RomÃ¡n.

----------------------------------

# ðŸ§  Deep Transformer para Series Temporales â€“ Influenza Prevalence Case

Este repositorio contiene la implementaciÃ³n y prueba del modelo **Transformer para series temporales**, me basÃ© en el paper:

> **"Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case"**  
> DOI: [10.48550/arXiv.2001.08317](https://doi.org/10.48550/arXiv.2001.08317) y ejecutÃ© el repositorio: [https://github.com/KasperGroesLudvigsen/influenza_transformer/](https://github.com/KasperGroesLudvigsen/influenza_transformer/))

TambiÃ©n contiene la implementaciÃ³n de nbeats y se hizo la comparativa de ambos modelos.

---

## ðŸŽ¯ Objetivo

Ahora, el objetivo, a parte de replicar el modelo propuesto en el artÃ­culo mencionado, evaluando su comportamiento inicial sobre datos de prevalencia de influenza, fue entrenar nbeats con el mismo dataset del paper original y comparar ambos modelos para ver un resultado.

---
## Consideraciones

Para la correcta ejecuciÃ³n de este repositorio, se implementÃ³ varios scripts que no son propios del repositorio original.
Se implementaron los script de `train.py` con el cual se generÃ³ el archivo ``transformer_timeseries_model.pth``; luego, se implementÃ³ `evaluate.py`, `data_utils.py` y hubo algunas modificaciones en los archivos `positional_encoder.py`, `dataset.py`.
A parte se implementaron los scripts `nbeats.py`, `trainnbeats.py` y `comparacion.py` (de ambos modelos).

-------------

ðŸ–¥ï¸ Entorno de ejecuciÃ³n:
- CPU

- Procesador: ( IntelÂ® Coreâ„¢ Ultra 7 265K) mejor procesador que la semana pasada 

- PyTorch

- Entrenamiento con 15 Ã©pocas (demorÃ³ menos tiempo que en el entrenamiento de 5 Ã©pocas de la semana pasada con el anterior procesador antiguo que tenÃ­a).



## ðŸ› ï¸ TecnologÃ­as usadas

- Python 3.10+
- PyTorch
- NumPy
- Matplotlib
- Pandas


-----------------

## ðŸ“ Archivos subidos: 
- train.py.

- evaluate.py.

- dataset.py.

- data_utils.py.

- inference.py.

- inference_example.py.

- positional_encoder.py.

- sandbox.py.
  
- utils.py.
- nbeats.py

- train_nbeats.py
  
- comparacion.py

- model / transformer_timeseries.py

> âš ï¸ No se subiÃ³ el archivo generado del entrenamiento`transformer_timeseries_model.pth`.
> âš ï¸ Tampoco se subiÃ³ el archivo generado del entrenamiento de nbeats `nbeats_model.pth`


> Captura de la estructura del proyecto:

> ![Captura de la estructura del proyecto](Img/estructura_2.jpg)

## ðŸ“‰ Resultados obtenidos con entrenamiento de 5 Ã©pocas del modelo transformers:

- MSE: 335281.37.

- MAE: 578.89.

- RMSE: 579.03.

## ðŸ“‰ Resultados obtenidos con entrenamiento de 15 Ã©pocas del modelo transformers:

- MSE: 1442712.349573.

- MAE: 1201.064511.

- RMSE: 1201.129614.


ðŸ“¸ Evidencias:

>Captura del entrenamiento de 5 Ã©pocas:
>![Captura del entrenamiento de 5 Ã©pocas](Img/entrenamiento_5_epocas.png)
>![Captura del entrenamiento de 5 Ã©pocas](Img/resultados_prediccion.png)


ðŸ“¸ Evidencias:

>Captura del entrenamiento de 15 Ã©pocas:
>![Captura del entrenamiento de 15 Ã©pocas](Img/entrenamiento_15_epocas.png)
>![Captura del entrenamiento de 15 Ã©pocas](Img/resultado_entrenamiento_15_epocas.jpeg)

 
>Captura del grÃ¡fico de comparaciÃ³n entre Transformers y nbeats:

> ![Captura del grÃ¡fico de predicciÃ³n](Img/comparacion_final.png)

Los resultados grÃ¡ficos muestran una comparaciÃ³n detallada entre las predicciones del modelo Transformer y N-BEATS frente a los valores reales (ground truth) en distintas muestras temporales. En general, se observa que N-BEATS presenta un mejor ajuste a la tendencia real, con predicciones mÃ¡s suaves y cercanas a los datos observados, especialmente en segmentos con patrones estacionales o cambios graduales. Por su parte, el Transformer, aunque captura la direcciÃ³n general de la serie, muestra mayores desviaciones en puntos crÃ­ticos (como picos o valles) y cierta inestabilidad en las transiciones, lo que sugiere dificultades para generalizar patrones complejos.

En muestras especÃ­ficas (por ejemplo, la Muestra 3), N-BEATS demuestra mayor precisiÃ³n al seguir fluctuaciones abruptas, mientras que el Transformer tiende a "rezagarse" o suavizar demasiado estas variaciones. Esto podrÃ­a deberse a la arquitectura de N-BEATS, que descompone explÃ­citamente la serie en componentes de tendencia y estacionalidad, permitiÃ©ndole adaptarse mejor a cambios rÃ¡pidos. Por el contrario, el Transformer, al depender de mecanismos de atenciÃ³n global, podrÃ­a estar sobreponderando ciertos pasos temporales o sufriendo de sobreajuste.




## ðŸ“š Referencia 

> **"Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case"**  
> DOI: [10.48550/arXiv.2001.08317](https://doi.org/10.48550/arXiv.2001.08317)

> Github original: [https://github.com/KasperGroesLudvigsen/influenza_transformer/](https://github.com/KasperGroesLudvigsen/influenza_transformer/)
